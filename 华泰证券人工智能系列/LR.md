##  知识点总结 ##
###  线性回归
关键字： 滚动训练集 主成分分析

本文主要关注并讨论了广义线性模型的如下几个环节：
1. 首先是模型选择的问题。除了传统的线性回归之外，逻辑回归、线性支持向量机等方
法同属于广义的线性模型，在业界有着相当广泛的应用。这些方法能否对多因子选股
的效果有进一步的提升？
2. 其次是正则化的问题。传统的线性回归模型中，在拟合回归方程这一步，我们不对参
数的取值范围做任何限定。然而在机器学习领域，最普遍的做法是引入正则化，对参
数的选择加以限制，防止过拟合的发生。现在流行的岭回归、 Lasso 回归和弹性网络
方法，正是将不同正则化方法和线性回归结合起来的产物。那么，在多因子选股模型
中，正则化是否有助于提升选股效果？
3. 再次是预处理方法的问题。 在多元线性回归中，因子共线性是需要尽力避免的问题。
消除因子共线性的方法之一是对多元变量做主成分分析，得到一组新的共线性程度较
小的变量。在多因子选股模型中，我们关心主成分分析是否有效，对模型有多大的提
升作用？
4. 最后是模型参数的问题。 多因子选股模型中包含一系列自由参数。 例如，对于 T+1 期
因子预期收益的估计通常需回溯前 N 期的历史收益， N 的取值多少最为合理？又如选
择不同正则化系数、不同损失函数，最终的选股效果是否存在差别？


-----------------
总结：
1. 正则化对选股效果没有明显的提升作用。 岭回归、 Lasso 回归和弹性网络的表现和线性
回归类似。  我们猜测有两个可能的原因。 首先， 样本的所有特征都是已被证明有效的因子，
因此我们的特征不具备稀疏性。 以 Lasso 回归为代表的 L1 正则化适用于从海量特征中选
出少数有效的特征， 因而我们面对的问题不符合 Lasso 的应用场景。 其次，由于预处理过程中做了去极值、 标准化和主成分分析， 在降低因子共线性的同时减少了极端样本的出现
概率， 因而进一步削弱了正则化的价值。 由于预处理过

 --------------
 疑问：
 1 . 逻辑回归的似然函数𝐿(𝑤⃗⃗ )为：𝐿(𝑤⃗⃗ ) = ∏𝑁 𝑖=1𝑃(𝑦𝑖 = 1|𝑥𝑖)𝑦𝑖(1 − 𝑃(𝑦𝑖 = 1|𝑥𝑖))1−𝑦𝑖
 
 何为似然函数？
 答：
 
 
 2 . 折页损失函数 hinge Loss（同 线性支持向量机）
 -  答：
 Hinge Loss 是机器学习领域中的一种损失函数，可用于“最大间隔(max-margin)”分类，
 其最著名的应用是作为SVM的目标函数。 在二分类情况下，公式如下： 

    L(y) = max(0 , 1 – t⋅y)

    其中，y是预测值(-1到1之间)，t为目标值(1或 -1)。
 其含义为，y的值在 -1到1之间即可，并不鼓励 |y|>1，
 即让某个样本能够正确分类就可以了，不鼓励分类器过度自信，
 当样本与分割线的距离超过1时并不会有任何奖励。
 目的在于使分类器更专注于整体的分类误差。
 
 3. 最小二乘法：
 答： 求试损失函数数值最小的x的方法就是用最小二乘法或者梯度下降法
 因为最小二乘法需要转化成矩阵乘法  所以尽量避免
 通常以最小化残差平方和（即最小二乘）为目标， 给出系数向量𝑤⃗⃗ 的一个线性无偏估计𝑤⃗⃗ = (𝑋T 𝑋)−1𝑦。
但是现实往往不是那么简单， 尤其在当今的大数据时代，当数据数量爆炸式增长时，反演
矩阵(𝑋T 𝑋)−1的大小将以样本数量平方的速度增长，对计算机的存储提出了很大的挑战